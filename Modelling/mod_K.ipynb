{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(23)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(980, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>pc</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>525</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>0.5</td>\n",
       "      <td>137</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>262</td>\n",
       "      <td>1587</td>\n",
       "      <td>1891</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>925</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0.2</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1262</td>\n",
       "      <td>1520</td>\n",
       "      <td>2466</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>1076</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.7</td>\n",
       "      <td>191</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1088</td>\n",
       "      <td>1718</td>\n",
       "      <td>2355</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>871</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0.1</td>\n",
       "      <td>178</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>194</td>\n",
       "      <td>1437</td>\n",
       "      <td>437</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>0.2</td>\n",
       "      <td>130</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>104</td>\n",
       "      <td>541</td>\n",
       "      <td>2829</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  \\\n",
       "1275            525     1          0.5         1   5       0          51   \n",
       "872             925     1          2.1         0   9       0          56   \n",
       "1321           1076     0          2.3         1   0       1          14   \n",
       "64              871     0          0.6         0   2       0          52   \n",
       "1070            894     0          0.9         0   5       1          54   \n",
       "\n",
       "      m_dep  mobile_wt  n_cores  pc  px_height  px_width   ram  sc_h  sc_w  \\\n",
       "1275    0.5        137        8  11        262      1587  1891    18     3   \n",
       "872     0.2        196        1  14       1262      1520  2466     8     0   \n",
       "1321    0.7        191        5   0       1088      1718  2355    17    16   \n",
       "64      0.1        178        3   3        194      1437   437    14     7   \n",
       "1070    0.2        130        3  15        104       541  2829    11     5   \n",
       "\n",
       "      talk_time  three_g  touch_screen  wifi  \n",
       "1275         12        0             1     0  \n",
       "872           5        1             0     0  \n",
       "1321          2        1             1     1  \n",
       "64           17        1             0     0  \n",
       "1070         13        1             0     1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobile_modelling = pd.read_csv('../Data/Data_modelling/mobile_modelling.csv')\n",
    "\n",
    "X = mobile_modelling.iloc[:, 0:-1]\n",
    "y = mobile_modelling.iloc[:, -1]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val, y_val, stratify=y_val, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "mobile_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "print(mobile_df.shape)\n",
    "\n",
    "X_train = mobile_df.iloc[:, 0:-1]\n",
    "y_train = mobile_df.iloc[:, -1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1062    3579\n",
       "607     1368\n",
       "742     2278\n",
       "337     2574\n",
       "550     3169\n",
       "Name: ram, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more than 0.02\n",
    "columns_to_drop_1 = ['touch_screen', 'blue', 'dual_sim', 'four_g', 'wifi', 'three_g']\n",
    "X_train_1 = X_train.drop(columns=columns_to_drop_1)\n",
    "X_val_1 = X_val.drop(columns=columns_to_drop_1)\n",
    "\n",
    "# more than 0.05\n",
    "columns_to_drop_2 = ['touch_screen', 'blue', 'dual_sim', 'four_g', 'wifi', 'three_g', 'fc', 'n_cores', 'm_dep', 'sc_h', 'clock_speed', 'sc_w', 'pc', 'talk_time', 'int_memory', 'mobile_wt']\n",
    "X_train_2 = X_train.drop(columns=columns_to_drop_2)\n",
    "X_val_2 = X_val.drop(columns=columns_to_drop_2)\n",
    "\n",
    "# just RAM\n",
    "X_train_3 = X_train['ram']\n",
    "X_val_3 = X_val['ram']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most_frequent:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.27      0.27        73\n",
      "           1       0.23      0.22      0.22        74\n",
      "           2       0.26      0.29      0.27        73\n",
      "           3       0.26      0.24      0.25        74\n",
      "\n",
      "    accuracy                           0.26       294\n",
      "   macro avg       0.25      0.26      0.25       294\n",
      "weighted avg       0.25      0.26      0.25       294\n",
      "\n",
      "stratified:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.26      0.26        73\n",
      "           1       0.31      0.30      0.30        74\n",
      "           2       0.30      0.32      0.31        73\n",
      "           3       0.31      0.30      0.31        74\n",
      "\n",
      "    accuracy                           0.29       294\n",
      "   macro avg       0.29      0.29      0.29       294\n",
      "weighted avg       0.29      0.29      0.29       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dc = DummyClassifier(strategy='uniform')\n",
    "dc.fit(X_train,y_train)\n",
    "y_pred = dc.predict(X_val)\n",
    "print('most_frequent:')\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "dc = DummyClassifier(strategy='stratified')\n",
    "dc.fit(X_train,y_train)\n",
    "y_pred = dc.predict(X_val)\n",
    "print('stratified:')\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.16      0.17        73\n",
      "           1       0.21      0.22      0.21        74\n",
      "           2       0.28      0.30      0.29        73\n",
      "           3       0.27      0.26      0.26        74\n",
      "\n",
      "    accuracy                           0.23       294\n",
      "   macro avg       0.23      0.23      0.23       294\n",
      "weighted avg       0.23      0.23      0.23       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create one-vs-rest logistic regression instance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(multi_class=\"ovr\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = dc.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.29      0.27        73\n",
      "           1       0.26      0.20      0.23        74\n",
      "           2       0.26      0.26      0.26        73\n",
      "           3       0.28      0.31      0.30        74\n",
      "\n",
      "    accuracy                           0.27       294\n",
      "   macro avg       0.26      0.27      0.26       294\n",
      "weighted avg       0.26      0.27      0.26       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(multi_class=\"ovr\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_1, y_train)\n",
    "y_pred_1 = dc.predict(X_val_1)\n",
    "\n",
    "print(classification_report(y_val, y_pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.22      0.21        73\n",
      "           1       0.31      0.30      0.30        74\n",
      "           2       0.30      0.30      0.30        73\n",
      "           3       0.17      0.16      0.16        74\n",
      "\n",
      "    accuracy                           0.24       294\n",
      "   macro avg       0.25      0.25      0.25       294\n",
      "weighted avg       0.25      0.24      0.25       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(multi_class=\"ovr\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_2, y_train)\n",
    "y_pred_2 = dc.predict(X_val_2)\n",
    "\n",
    "print(classification_report(y_val, y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1891. 2466. 2355.  437. 2829.  468. 1017. 1817. 3035.  931. 2514.  832.\n 1365. 3448.  957.  422. 2926. 3029. 3209. 3673. 3772.  841. 1958. 2803.\n  908. 3104. 1449.  796.  431. 2273.  651.  292. 1641. 2855.  461. 1277.\n 1053. 2944. 2013. 2608.  435. 1457. 2358. 3227.  515. 1122. 3705.  719.\n  941.  874. 1152. 2677. 2986. 2991. 1112. 2438.  920. 3153. 3105. 3299.\n 3197. 1277.  532.  456. 3585. 2349.  891. 3704. 3167.  574.  783. 2644.\n  284. 3473. 3630. 1607. 1419. 2173. 2315. 1601.  916. 2124. 3833. 2511.\n 2598. 2190. 1326.  851. 1652. 2498. 2385.  722. 1411. 3506. 3784. 2603.\n 2973. 3770. 2111. 2317. 3615.  665. 1300. 2598.  985.  586.  708. 2927.\n 1028. 2746. 1875. 2262.  720. 3586. 3862. 3703. 1947.  509. 2614. 2074.\n 3993. 3952. 2169. 3937. 3713.  424. 3996. 1489. 3672. 2622. 2256. 3565.\n  593. 3283. 2573. 1300.  941. 2674.  277. 3717. 2019. 1276. 3762. 1633.\n 1046. 1751. 3278.  966. 3883.  864. 1905. 2597. 2885.  256. 1282.  387.\n 3315. 3869.  933. 3607. 2080. 3019. 1275. 1406. 2962. 2993. 1486. 2583.\n  436. 2929. 3914. 2173. 1480. 1352. 2678. 3248.  343. 1337. 1005.  815.\n 2521. 2462. 2296. 3647. 3684.  769. 2049.  878. 3176.  550. 2746.  927.\n 1907. 2296. 2389. 3608. 1185. 2322. 2268. 3011. 2295. 1052.  432.  584.\n 1641. 2819. 2048. 2009.  857. 3532. 2253. 1418.  595.  323.  700. 3397.\n 3233. 1769. 3623. 2359. 1747. 3358.  591. 1234.  961. 2493. 3480. 3442.\n 1394. 2406. 2103. 1797. 1214. 3606. 2908. 3576. 2940. 1598. 1619. 2064.\n  793.  343. 3600. 2734. 2016. 2197.  392.  402. 3593. 1301. 3809.  869.\n 3139. 2183. 2981. 3739.  508. 2981. 1862. 1324. 2819. 3265. 3086. 1892.\n 2324. 3900. 1018.  258. 2030. 3918. 1180.  881.  336. 1400. 2746. 1593.\n 2800. 1470. 3538. 1545. 2858. 1971. 2259.  262. 2572.  702. 2711. 2053.\n 2600. 2085. 2243.  435. 3153. 1432. 2478.  732. 3260.  666.  716. 1334.\n 1184. 3886. 3272. 2583. 1653. 2784. 2973. 1172. 3796.  424. 1742. 1667.\n 2806. 2337. 2893. 2339.  587.  790. 2390. 2351. 2189. 1462. 3242. 1900.\n  490. 3139. 1615. 1732. 2571. 1642. 1457. 3681. 1724. 2802. 2381.  841.\n 2391.  506. 3132. 2669. 1401. 1974. 2216.  892. 2693.  495. 3469.  302.\n 3142. 3119. 2479. 2372.  463. 2445. 2953. 2132. 3210. 3654.  315. 1175.\n 3063.  438. 2335.  315. 2610. 1179. 1074. 1656.  814. 3799. 2581. 2332.\n 1711. 3383. 3755. 3510. 3215.  546. 1887.  504. 1610.  418.  471. 2377.\n 3685. 3796. 2457. 3927.  785. 1702. 2509. 2940.  797. 2505. 3885. 3726.\n 1247.  368. 1587. 1321. 3803. 3057. 1663. 3015. 1713. 3865. 1788. 2039.\n  820. 3899.  850. 3396. 1212. 1122. 2392. 3963. 1105. 1287. 2651. 1419.\n 1022. 2004. 3915. 3271. 3183. 1204. 1617. 2719. 2334.  465. 3484. 1348.\n 3872. 3490.  505. 2855. 1051. 1391. 3006. 1667. 3930. 2915. 3564. 2648.\n 2933. 3169. 3323. 3087. 3945. 2462. 2248. 1086. 1241. 2144.  504. 3115.\n 3774.  629.  503.  354. 2022. 3964.  639. 1389. 1663. 2362. 2110.  706.\n 3941. 3423. 2341. 1725. 3978. 2870. 1620. 1945. 2575. 3695. 1938.  876.\n  663. 1656.  568. 3637.  714. 2114. 2413. 2863. 1877. 2336.  297. 1998.\n 2908. 1280. 1019. 2872. 1211. 3970.  392.  861. 3771. 1813. 3563. 3933.\n 2343. 2376. 3220. 2190.  643. 3012. 3285. 3902. 2948. 2965. 1513. 3785.\n 1726. 3132. 2200. 1620. 1441. 1704. 3566. 2078.  759. 2056.  794.  967.\n 2001. 3518. 1285. 1665. 2496. 1622. 1205. 1382. 1183. 2403. 3534. 3416.\n 2528. 1164.  770.  543. 3868. 3059.  952. 1339. 1027. 1637.  733. 2261.\n 1220. 2800. 1913. 1780. 3458. 3297. 2871. 3351. 3884. 3801. 3720. 2243.\n  783.  524.  267.  929. 3076. 1369. 2801. 1377. 1222. 2552. 2373. 2166.\n 2312. 2290. 3615.  824. 3242. 2948. 3685. 1060. 1115. 2661. 3355. 3955.\n 3707.  523.  988. 1903. 1445. 1686. 1655. 1244.  978.  348. 2915.  735.\n 1073.  403. 3869.  318.  440. 2984. 3835.  340. 3376. 3397. 3362. 2727.\n 2299. 3094. 2865.  291. 1470. 1214. 2301. 1086. 3744. 3702. 3122.  485.\n 2023. 1955. 3601. 1950. 2201. 3419.  560.  838. 2641. 1695. 3406. 1193.\n  670. 3187. 3333. 1175.  990. 3097.  378. 3961. 2547. 2484. 1930. 2944.\n 2958. 2249.  258. 2369.  336. 2524. 2658. 1366. 2637. 1704. 3212.  770.\n 3904. 2438. 1303. 2633. 3208. 3021.  819. 1295. 3860. 1958. 2227. 2488.\n 1322. 1756. 2518. 2278. 1861. 1403. 2977. 3946. 1152. 2735. 3204. 3537.\n  584.  582.  373.  860. 1675. 3358. 2156. 3872. 3564. 3845. 3629. 1948.\n 1970. 2801. 2969. 1624.  688. 2488.  905. 3577. 3817. 1550. 1938. 1201.\n 2107. 2167. 2847. 2674. 1459. 2027.  696. 3568. 1379.  311. 3206. 1590.\n 2982. 3302. 1044. 3330.  263. 2082.  316. 1251. 3210.  712. 1422. 3078.\n 1731. 3472. 3905. 1571. 1308. 2941. 1384. 1837. 2219. 1591. 2517. 3317.\n  726. 2096.  880.  475.  990. 3878.  457. 2520. 3648. 1464. 2769. 2842.\n 2635. 1125. 2560.  552. 3100. 3760.  820. 3571. 2655. 1036. 3892. 2951.\n  667. 3704. 3948. 3396. 1270. 1454. 3731. 2385. 2812.  478. 1277. 3941.\n 2311. 1832. 2382. 1713. 3969. 2627. 3264. 3703. 1724.  752.  347. 1518.\n  545. 3660. 1958. 1436. 3962. 3644. 3612. 1812. 2857. 2339.  763. 3429.\n 3834.  514.  774. 2542. 1223. 2676. 3745. 2487. 2073.  685. 1326. 2048.\n  862. 3242. 1561.  980.  907. 3230. 1494. 1853. 2889.  841. 1181. 3856.\n 2392. 1851. 3761. 1305. 2473. 1687. 3597. 2029. 2378. 1444. 2895. 2705.\n 3185. 1762.  604.  511. 3165. 3162. 1360. 3548.  398.  728. 3925. 3497.\n 2532. 1393. 1412. 2750. 3709. 1076.  467. 2394. 3693. 2316. 1223.  668.\n 3486. 2942. 3991. 1032. 2616. 2870.  527. 3068. 3696.  485. 1362. 1356.\n 3033.  582. 1595. 2577. 3501. 1968. 2829. 2458. 3632. 2003. 2060.  473.\n 1254. 3587.  707. 1341. 3786. 1835. 1701. 3676. 3372. 2083. 1083.  429.\n 3922. 1974.  417. 1246. 3483. 3573. 1898. 2790. 1906. 2033. 1653. 1824.\n  799. 1906. 1882. 1891.  404. 1309.  411. 2268. 1571. 1919. 1142. 1774.\n 3411. 3940. 1404. 1799.  470. 3736. 2052. 1999. 3421. 2967.  947.  659.\n 1604.  513. 3155. 3859. 3056. 3800. 2298.  606. 2215. 1412. 3773. 3300.\n 2302. 2113. 3709.  616. 2822. 1699. 1475. 3142.  999. 1386. 2974.  854.\n  331. 2137. 2050. 3619. 1336. 2084. 2304. 3352. 2965. 1338. 1214.  337.\n 3654.  398. 3625.  893. 1343. 2916. 1735. 2236.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d1/hw8k8s1x019_54t0zwhhcrxr0000gp/T/ipykernel_4621/3955778646.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_pred_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1506\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m   1509\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    770\u001b[0m                     \u001b[0;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1891. 2466. 2355.  437. 2829.  468. 1017. 1817. 3035.  931. 2514.  832.\n 1365. 3448.  957.  422. 2926. 3029. 3209. 3673. 3772.  841. 1958. 2803.\n  908. 3104. 1449.  796.  431. 2273.  651.  292. 1641. 2855.  461. 1277.\n 1053. 2944. 2013. 2608.  435. 1457. 2358. 3227.  515. 1122. 3705.  719.\n  941.  874. 1152. 2677. 2986. 2991. 1112. 2438.  920. 3153. 3105. 3299.\n 3197. 1277.  532.  456. 3585. 2349.  891. 3704. 3167.  574.  783. 2644.\n  284. 3473. 3630. 1607. 1419. 2173. 2315. 1601.  916. 2124. 3833. 2511.\n 2598. 2190. 1326.  851. 1652. 2498. 2385.  722. 1411. 3506. 3784. 2603.\n 2973. 3770. 2111. 2317. 3615.  665. 1300. 2598.  985.  586.  708. 2927.\n 1028. 2746. 1875. 2262.  720. 3586. 3862. 3703. 1947.  509. 2614. 2074.\n 3993. 3952. 2169. 3937. 3713.  424. 3996. 1489. 3672. 2622. 2256. 3565.\n  593. 3283. 2573. 1300.  941. 2674.  277. 3717. 2019. 1276. 3762. 1633.\n 1046. 1751. 3278.  966. 3883.  864. 1905. 2597. 2885.  256. 1282.  387.\n 3315. 3869.  933. 3607. 2080. 3019. 1275. 1406. 2962. 2993. 1486. 2583.\n  436. 2929. 3914. 2173. 1480. 1352. 2678. 3248.  343. 1337. 1005.  815.\n 2521. 2462. 2296. 3647. 3684.  769. 2049.  878. 3176.  550. 2746.  927.\n 1907. 2296. 2389. 3608. 1185. 2322. 2268. 3011. 2295. 1052.  432.  584.\n 1641. 2819. 2048. 2009.  857. 3532. 2253. 1418.  595.  323.  700. 3397.\n 3233. 1769. 3623. 2359. 1747. 3358.  591. 1234.  961. 2493. 3480. 3442.\n 1394. 2406. 2103. 1797. 1214. 3606. 2908. 3576. 2940. 1598. 1619. 2064.\n  793.  343. 3600. 2734. 2016. 2197.  392.  402. 3593. 1301. 3809.  869.\n 3139. 2183. 2981. 3739.  508. 2981. 1862. 1324. 2819. 3265. 3086. 1892.\n 2324. 3900. 1018.  258. 2030. 3918. 1180.  881.  336. 1400. 2746. 1593.\n 2800. 1470. 3538. 1545. 2858. 1971. 2259.  262. 2572.  702. 2711. 2053.\n 2600. 2085. 2243.  435. 3153. 1432. 2478.  732. 3260.  666.  716. 1334.\n 1184. 3886. 3272. 2583. 1653. 2784. 2973. 1172. 3796.  424. 1742. 1667.\n 2806. 2337. 2893. 2339.  587.  790. 2390. 2351. 2189. 1462. 3242. 1900.\n  490. 3139. 1615. 1732. 2571. 1642. 1457. 3681. 1724. 2802. 2381.  841.\n 2391.  506. 3132. 2669. 1401. 1974. 2216.  892. 2693.  495. 3469.  302.\n 3142. 3119. 2479. 2372.  463. 2445. 2953. 2132. 3210. 3654.  315. 1175.\n 3063.  438. 2335.  315. 2610. 1179. 1074. 1656.  814. 3799. 2581. 2332.\n 1711. 3383. 3755. 3510. 3215.  546. 1887.  504. 1610.  418.  471. 2377.\n 3685. 3796. 2457. 3927.  785. 1702. 2509. 2940.  797. 2505. 3885. 3726.\n 1247.  368. 1587. 1321. 3803. 3057. 1663. 3015. 1713. 3865. 1788. 2039.\n  820. 3899.  850. 3396. 1212. 1122. 2392. 3963. 1105. 1287. 2651. 1419.\n 1022. 2004. 3915. 3271. 3183. 1204. 1617. 2719. 2334.  465. 3484. 1348.\n 3872. 3490.  505. 2855. 1051. 1391. 3006. 1667. 3930. 2915. 3564. 2648.\n 2933. 3169. 3323. 3087. 3945. 2462. 2248. 1086. 1241. 2144.  504. 3115.\n 3774.  629.  503.  354. 2022. 3964.  639. 1389. 1663. 2362. 2110.  706.\n 3941. 3423. 2341. 1725. 3978. 2870. 1620. 1945. 2575. 3695. 1938.  876.\n  663. 1656.  568. 3637.  714. 2114. 2413. 2863. 1877. 2336.  297. 1998.\n 2908. 1280. 1019. 2872. 1211. 3970.  392.  861. 3771. 1813. 3563. 3933.\n 2343. 2376. 3220. 2190.  643. 3012. 3285. 3902. 2948. 2965. 1513. 3785.\n 1726. 3132. 2200. 1620. 1441. 1704. 3566. 2078.  759. 2056.  794.  967.\n 2001. 3518. 1285. 1665. 2496. 1622. 1205. 1382. 1183. 2403. 3534. 3416.\n 2528. 1164.  770.  543. 3868. 3059.  952. 1339. 1027. 1637.  733. 2261.\n 1220. 2800. 1913. 1780. 3458. 3297. 2871. 3351. 3884. 3801. 3720. 2243.\n  783.  524.  267.  929. 3076. 1369. 2801. 1377. 1222. 2552. 2373. 2166.\n 2312. 2290. 3615.  824. 3242. 2948. 3685. 1060. 1115. 2661. 3355. 3955.\n 3707.  523.  988. 1903. 1445. 1686. 1655. 1244.  978.  348. 2915.  735.\n 1073.  403. 3869.  318.  440. 2984. 3835.  340. 3376. 3397. 3362. 2727.\n 2299. 3094. 2865.  291. 1470. 1214. 2301. 1086. 3744. 3702. 3122.  485.\n 2023. 1955. 3601. 1950. 2201. 3419.  560.  838. 2641. 1695. 3406. 1193.\n  670. 3187. 3333. 1175.  990. 3097.  378. 3961. 2547. 2484. 1930. 2944.\n 2958. 2249.  258. 2369.  336. 2524. 2658. 1366. 2637. 1704. 3212.  770.\n 3904. 2438. 1303. 2633. 3208. 3021.  819. 1295. 3860. 1958. 2227. 2488.\n 1322. 1756. 2518. 2278. 1861. 1403. 2977. 3946. 1152. 2735. 3204. 3537.\n  584.  582.  373.  860. 1675. 3358. 2156. 3872. 3564. 3845. 3629. 1948.\n 1970. 2801. 2969. 1624.  688. 2488.  905. 3577. 3817. 1550. 1938. 1201.\n 2107. 2167. 2847. 2674. 1459. 2027.  696. 3568. 1379.  311. 3206. 1590.\n 2982. 3302. 1044. 3330.  263. 2082.  316. 1251. 3210.  712. 1422. 3078.\n 1731. 3472. 3905. 1571. 1308. 2941. 1384. 1837. 2219. 1591. 2517. 3317.\n  726. 2096.  880.  475.  990. 3878.  457. 2520. 3648. 1464. 2769. 2842.\n 2635. 1125. 2560.  552. 3100. 3760.  820. 3571. 2655. 1036. 3892. 2951.\n  667. 3704. 3948. 3396. 1270. 1454. 3731. 2385. 2812.  478. 1277. 3941.\n 2311. 1832. 2382. 1713. 3969. 2627. 3264. 3703. 1724.  752.  347. 1518.\n  545. 3660. 1958. 1436. 3962. 3644. 3612. 1812. 2857. 2339.  763. 3429.\n 3834.  514.  774. 2542. 1223. 2676. 3745. 2487. 2073.  685. 1326. 2048.\n  862. 3242. 1561.  980.  907. 3230. 1494. 1853. 2889.  841. 1181. 3856.\n 2392. 1851. 3761. 1305. 2473. 1687. 3597. 2029. 2378. 1444. 2895. 2705.\n 3185. 1762.  604.  511. 3165. 3162. 1360. 3548.  398.  728. 3925. 3497.\n 2532. 1393. 1412. 2750. 3709. 1076.  467. 2394. 3693. 2316. 1223.  668.\n 3486. 2942. 3991. 1032. 2616. 2870.  527. 3068. 3696.  485. 1362. 1356.\n 3033.  582. 1595. 2577. 3501. 1968. 2829. 2458. 3632. 2003. 2060.  473.\n 1254. 3587.  707. 1341. 3786. 1835. 1701. 3676. 3372. 2083. 1083.  429.\n 3922. 1974.  417. 1246. 3483. 3573. 1898. 2790. 1906. 2033. 1653. 1824.\n  799. 1906. 1882. 1891.  404. 1309.  411. 2268. 1571. 1919. 1142. 1774.\n 3411. 3940. 1404. 1799.  470. 3736. 2052. 1999. 3421. 2967.  947.  659.\n 1604.  513. 3155. 3859. 3056. 3800. 2298.  606. 2215. 1412. 3773. 3300.\n 2302. 2113. 3709.  616. 2822. 1699. 1475. 3142.  999. 1386. 2974.  854.\n  331. 2137. 2050. 3619. 1336. 2084. 2304. 3352. 2965. 1338. 1214.  337.\n 3654.  398. 3625.  893. 1343. 2916. 1735. 2236.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(multi_class=\"ovr\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_3, y_train)\n",
    "y_pred_3 = dc.predict(X_val_3)\n",
    "\n",
    "print(classification_report(y_val, y_pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
