{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natal\\AppData\\Local\\Temp\\ipykernel_16716\\3504193263.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_modelling = pd.read_csv('../Data/Data_modelling/mobile_modelling.csv')\n",
    "\n",
    "X = mobile_modelling.iloc[:, 0:-1]\n",
    "y = mobile_modelling.iloc[:, -1]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val, y_val, stratify=y_val, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "mobile_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# variables with importance of more than 0.02 \n",
    "columns_to_drop_1 = ['touch_screen', 'blue', 'dual_sim', 'four_g', 'wifi', 'three_g']\n",
    "X_train_r = X_train.drop(columns=columns_to_drop_1)\n",
    "X_val_r = X_val.drop(columns=columns_to_drop_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging (Bootstrap Aggregating)\n",
    "designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It decreases the variance and helps to avoid overfitting. It is usually applied to decision tree methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we're testing Bagging method with two tree models - DecisionTreeClassifier and RandomForestClassifier\n",
    "\n",
    "in 2 variants: with all the columns and with columns having importance greater than 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging with DecisionTreeClassifier - all columns\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94        73\n",
      "           1       0.78      0.84      0.81        74\n",
      "           2       0.70      0.79      0.74        73\n",
      "           3       0.94      0.78      0.85        74\n",
      "\n",
      "    accuracy                           0.83       294\n",
      "   macro avg       0.84      0.83      0.84       294\n",
      "weighted avg       0.84      0.83      0.84       294\n",
      "\n",
      "Bagging with DecisionTreeClassifier - important columns\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92        73\n",
      "           1       0.79      0.81      0.80        74\n",
      "           2       0.72      0.82      0.77        73\n",
      "           3       0.94      0.81      0.87        74\n",
      "\n",
      "    accuracy                           0.84       294\n",
      "   macro avg       0.84      0.84      0.84       294\n",
      "weighted avg       0.84      0.84      0.84       294\n",
      "\n",
      "\n",
      "Bagging with RandomForestClassifier - all columns\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93        73\n",
      "           1       0.75      0.77      0.76        74\n",
      "           2       0.72      0.74      0.73        73\n",
      "           3       0.93      0.86      0.90        74\n",
      "\n",
      "    accuracy                           0.83       294\n",
      "   macro avg       0.83      0.83      0.83       294\n",
      "weighted avg       0.83      0.83      0.83       294\n",
      "\n",
      "\n",
      "Bagging with RandomForestClassifier - important columns\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        73\n",
      "           1       0.77      0.80      0.78        74\n",
      "           2       0.74      0.77      0.75        73\n",
      "           3       0.94      0.86      0.90        74\n",
      "\n",
      "    accuracy                           0.84       294\n",
      "   macro avg       0.84      0.84      0.84       294\n",
      "weighted avg       0.84      0.84      0.84       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating 2 models\n",
    "model_dt = DecisionTreeClassifier(random_state=42)\n",
    "model_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Bagging for first model\n",
    "bagging_model_dt = BaggingClassifier(estimator=model_dt, n_estimators=10, random_state=42)\n",
    "bagging_model_dt.fit(X_train, y_train)\n",
    "\n",
    "bagging_model_dt_r = BaggingClassifier(estimator=model_dt, n_estimators=10, random_state=42)\n",
    "bagging_model_dt_r.fit(X_train_r, y_train)\n",
    "\n",
    "# Bagging for second model\n",
    "bagging_model_rf = BaggingClassifier(estimator=model_rf, n_estimators=10, random_state=42)\n",
    "bagging_model_rf.fit(X_train, y_train)\n",
    "\n",
    "bagging_model_rf_r = BaggingClassifier(estimator=model_rf, n_estimators=10, random_state=42)\n",
    "bagging_model_rf_r.fit(X_train_r, y_train)\n",
    "\n",
    "# Checking predictions on validation datasets\n",
    "predictions_model_dt = bagging_model_dt.predict(X_val)\n",
    "predictions_model_dt_r = bagging_model_dt_r.predict(X_val_r)\n",
    "\n",
    "predictions_model_rf = bagging_model_rf.predict(X_val)\n",
    "predictions_model_rf_r = bagging_model_rf_r.predict(X_val_r)\n",
    "\n",
    "# Printing classification report\n",
    "print(\"Bagging with DecisionTreeClassifier - all columns\")\n",
    "print(classification_report(y_val, predictions_model_dt))\n",
    "\n",
    "print(\"Bagging with DecisionTreeClassifier - important columns\")\n",
    "print(classification_report(y_val, predictions_model_dt_r))\n",
    "\n",
    "print(\"\\nBagging with RandomForestClassifier - all columns\")\n",
    "print(classification_report(y_val, predictions_model_rf))\n",
    "\n",
    "print(\"\\nBagging with RandomForestClassifier - important columns\")\n",
    "print(classification_report(y_val, predictions_model_rf_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "we're checking Boosting method with:\n",
    "    AdaBoost\n",
    "    XGBoost\n",
    "    CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO ogarnąć xgboost i catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost\n",
    "#!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost for all columns:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.72      0.81        32\n",
      "           1       0.66      0.68      0.67        31\n",
      "           2       0.40      0.72      0.52        32\n",
      "           3       0.33      0.13      0.19        31\n",
      "\n",
      "    accuracy                           0.56       126\n",
      "   macro avg       0.58      0.56      0.54       126\n",
      "weighted avg       0.58      0.56      0.55       126\n",
      "\n",
      "\n",
      "AdaBoost for selected columns:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.69        73\n",
      "           1       0.60      0.74      0.67        74\n",
      "           2       0.41      0.68      0.52        73\n",
      "           3       0.41      0.19      0.26        74\n",
      "\n",
      "    accuracy                           0.55       294\n",
      "   macro avg       0.58      0.55      0.53       294\n",
      "weighted avg       0.58      0.55      0.53       294\n",
      "\n",
      "\n",
      "XGBoost for all columns:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95        32\n",
      "           1       0.87      0.87      0.87        31\n",
      "           2       0.91      0.91      0.91        32\n",
      "           3       0.97      1.00      0.98        31\n",
      "\n",
      "    accuracy                           0.93       126\n",
      "   macro avg       0.93      0.93      0.93       126\n",
      "weighted avg       0.93      0.93      0.93       126\n",
      "\n",
      "\n",
      "XGBoost for selected columns:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96        73\n",
      "           1       0.86      0.88      0.87        74\n",
      "           2       0.84      0.85      0.84        73\n",
      "           3       0.95      0.93      0.94        74\n",
      "\n",
      "    accuracy                           0.90       294\n",
      "   macro avg       0.90      0.90      0.90       294\n",
      "weighted avg       0.90      0.90      0.90       294\n",
      "\n",
      "\n",
      "CatBoost for all columns:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95        32\n",
      "           1       0.90      0.84      0.87        31\n",
      "           2       0.88      0.94      0.91        32\n",
      "           3       0.97      1.00      0.98        31\n",
      "\n",
      "    accuracy                           0.93       126\n",
      "   macro avg       0.93      0.93      0.93       126\n",
      "weighted avg       0.93      0.93      0.93       126\n",
      "\n",
      "\n",
      "CatBoost for selected columns:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98        73\n",
      "           1       0.89      0.92      0.91        74\n",
      "           2       0.85      0.88      0.86        73\n",
      "           3       0.96      0.92      0.94        74\n",
      "\n",
      "    accuracy                           0.92       294\n",
      "   macro avg       0.92      0.92      0.92       294\n",
      "weighted avg       0.92      0.92      0.92       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function for training and evaluating the model\n",
    "def train_and_evaluate_model(model, X_train, X_val, y_train, y_val):\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_val)\n",
    "    print(classification_report(y_val, predictions))\n",
    "\n",
    "# Modeling with AdaBoost for all columns\n",
    "print(\"AdaBoost for all columns:\")\n",
    "ada_model = AdaBoostClassifier(random_state=42, algorithm='SAMME')\n",
    "train_and_evaluate_model(ada_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Modeling with AdaBoost for selected columns\n",
    "print(\"\\nAdaBoost for selected columns:\")\n",
    "train_and_evaluate_model(ada_model, X_train_r, X_val_r, y_train, y_val)\n",
    "\n",
    "# Modeling with XGBoost for all columns\n",
    "print(\"\\nXGBoost for all columns:\")\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "train_and_evaluate_model(xgb_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Modeling with XGBoost for selected columns\n",
    "print(\"\\nXGBoost for selected columns:\")\n",
    "train_and_evaluate_model(xgb_model, X_train_r, X_val_r, y_train, y_val)\n",
    "\n",
    "# Modeling with CatBoost for all columns\n",
    "print(\"\\nCatBoost for all columns:\")\n",
    "cat_model = CatBoostClassifier(random_state=42, verbose=0)\n",
    "train_and_evaluate_model(cat_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Modeling with CatBoost for selected columns\n",
    "print(\"\\nCatBoost for selected columns:\")\n",
    "train_and_evaluate_model(cat_model, X_train_r, X_val_r, y_train, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-optimize\n",
      "  Downloading scikit_optimize-0.10.1-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\natal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-optimize) (1.3.2)\n",
      "Collecting pyaml>=16.9 (from scikit-optimize)\n",
      "  Downloading pyaml-23.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\natal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-optimize) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\natal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-optimize) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\natal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-optimize) (1.4.1.post1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\natal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-optimize) (23.2)\n",
      "Collecting PyYAML (from pyaml>=16.9->scikit-optimize)\n",
      "  Downloading PyYAML-6.0.1-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\natal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.3.0)\n",
      "Downloading scikit_optimize-0.10.1-py2.py3-none-any.whl (107 kB)\n",
      "   ---------------------------------------- 0.0/107.7 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 61.4/107.7 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 107.7/107.7 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading pyaml-23.12.0-py3-none-any.whl (23 kB)\n",
      "Downloading PyYAML-6.0.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "   ---------------------------------------- 0.0/138.7 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 122.9/138.7 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 138.7/138.7 kB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: PyYAML, pyaml, scikit-optimize\n",
      "Successfully installed PyYAML-6.0.1 pyaml-23.12.0 scikit-optimize-0.10.1\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val. score: 0.9653070924247825\n",
      "test score: 0.9693877551020408\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    SVC(),\n",
    "    {\n",
    "        'degree': (1, 8),  # integer valued parameter\n",
    "        'kernel': ['linear', 'poly', 'rbf'],  # categorical parameter\n",
    "    },\n",
    "    n_iter=10,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "print(\"val. score: %s\" % opt.best_score_)\n",
    "print(\"test score: %s\" % opt.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - val. score: 0.86024027066409\n",
      "Random Forest - test score: 0.8707482993197279\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer with BayesSearchCV\n",
    "opt_rf = BayesSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    {\n",
    "        'n_estimators': (10, 1000),  # Number of trees in the forest\n",
    "        'max_depth': (1, 20),  # Maximum depth of the tree\n",
    "        'min_samples_split': (2, 10),  # Minimum number of samples required to split a node\n",
    "        'min_samples_leaf': (1, 10),  # Minimum number of samples required at each leaf node\n",
    "        'max_features': (1, len(X_train.columns)),  # Number of features to consider when looking for the best split\n",
    "    },\n",
    "    n_iter=10,  # Number of optimization iterations\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "# Fit the optimizer to the training data\n",
    "opt_rf.fit(X_train, y_train)\n",
    "\n",
    "# Print the best validation score for Random Forest\n",
    "print(\"Random Forest - val. score: %s\" % opt_rf.best_score_)\n",
    "\n",
    "# Print the test score for Random Forest\n",
    "print(\"Random Forest - test score: %s\" % opt_rf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - val. score: 0.824499853035903\n",
      "Decision Tree - test score: 0.8299319727891157\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer with BayesSearchCV\n",
    "opt_dt = BayesSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    {\n",
    "        'max_depth': (1, 20),  # Maximum depth of the tree\n",
    "        'min_samples_split': (2, 10),  # Minimum number of samples required to split a node\n",
    "        'min_samples_leaf': (1, 10),  # Minimum number of samples required at each leaf node\n",
    "        'max_features': (1, len(X_train.columns)),  # Number of features to consider when looking for the best split\n",
    "    },\n",
    "    n_iter=10,  # Number of optimization iterations\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "# Fit the optimizer to the training data\n",
    "opt_dt.fit(X_train, y_train)\n",
    "\n",
    "# Print the best validation score for Decision Tree\n",
    "print(\"Decision Tree - val. score: %s\" % opt_dt.best_score_)\n",
    "\n",
    "# Print the test score for Decision Tree\n",
    "print(\"Decision Tree - test score: %s\" % opt_dt.score(X_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
