{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(23)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_modelling = pd.read_csv('../Data/Data_modelling/mobile_modelling.csv')\n",
    "\n",
    "X = mobile_modelling.iloc[:, 0:-1]\n",
    "y = mobile_modelling.iloc[:, -1]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val, y_val, stratify=y_val, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "mobile_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# more than 0.02\n",
    "columns_to_drop_1 = ['touch_screen', 'blue', 'dual_sim', 'four_g', 'wifi', 'three_g']\n",
    "X_train_r = X_train.drop(columns=columns_to_drop_1)\n",
    "X_val_r = X_val.drop(columns=columns_to_drop_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# used models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model1 = DecisionTreeClassifier(random_state=1)\n",
    "model2 = RandomForestClassifier()\n",
    "model3 = SVC(random_state=1, max_iter=1000, probability=True)\n",
    "model33 = SVC(random_state=1, max_iter=1000)\n",
    "model4 = KNeighborsClassifier()\n",
    "estimators=[('DecisionTree', model1), ('RandomForest', model2), ('SVC', model33)]\n",
    "estimators1=[('RandomForest', model2), ('KNN', model4), ('SVC', model3)]\n",
    "estimators2=[('DecisionTree', model1), ('RandomForest', model2),('SVC', model3), ('KNN', model4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into consideration our first modelling and the results we obtained we plan to take into consideration 4 simple models that performed well on our data:\n",
    "- SVC\n",
    "- Decision Tree\n",
    "- Random Forest Classifier\n",
    "- KNN \n",
    "\n",
    "We will use soft voting as well as hard voting. We will also try to find the best weight combinations for chosen models combination since we think that focusing on the model with the best performance may be beneficial. We will check the results for data without removed columns and with removed columns that have feature importance greater than 0,02."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works for 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for whole dataset:  0.9047619047619048\n",
      "accuracy for dataset without chosen columns:  0.8945578231292517\n"
     ]
    }
   ],
   "source": [
    "model_hard = VotingClassifier(estimators=estimators, voting='hard')\n",
    "model_hard.fit(X_train,y_train)\n",
    "y_hat = model_hard.predict(X_val)\n",
    "print('accuracy for whole dataset: ', accuracy_score(y_val, y_hat))\n",
    "\n",
    "model_hardr = VotingClassifier(estimators=estimators, voting='hard')\n",
    "model_hardr.fit(X_train_r,y_train)\n",
    "y_hat = model_hardr.predict(X_val_r)\n",
    "print('accuracy for dataset without chosen columns: ', accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for whole dataset:  0.9149659863945578\n",
      "accuracy for dataset without chosen columns:  0.9013605442176871\n"
     ]
    }
   ],
   "source": [
    "model_hard = VotingClassifier(estimators=estimators2, voting='hard')\n",
    "model_hard.fit(X_train,y_train)\n",
    "y_hat = model_hard.predict(X_val)\n",
    "print('accuracy for whole dataset: ', accuracy_score(y_val, y_hat))\n",
    "\n",
    "model_hardr = VotingClassifier(estimators=estimators2, voting='hard')\n",
    "model_hardr.fit(X_train_r,y_train)\n",
    "y_hat = model_hardr.predict(X_val_r)\n",
    "print('accuracy for dataset without chosen columns: ', accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for whole dataset:  0.9285714285714286\n",
      "accuracy for dataset without chosen columns:  0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "model_soft = VotingClassifier(estimators=estimators1, voting='soft')\n",
    "model_soft.fit(X_train,y_train)\n",
    "y_hat = model_soft.predict(X_val)\n",
    "print('accuracy for whole dataset: ', accuracy_score(y_val, y_hat))\n",
    "\n",
    "model_softr = VotingClassifier(estimators=estimators1, voting='soft')\n",
    "model_softr.fit(X_train_r,y_train)\n",
    "y_hat = model_softr.predict(X_val_r)\n",
    "print('accuracy for dataset without chosen columns: ', accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for whole dataset:  0.9115646258503401\n",
      "accuracy for dataset without chosen columns:  0.9319727891156463\n"
     ]
    }
   ],
   "source": [
    "model_soft = VotingClassifier(estimators=estimators2, voting='soft')\n",
    "model_soft.fit(X_train,y_train)\n",
    "y_hat = model_soft.predict(X_val)\n",
    "print('accuracy for whole dataset: ', accuracy_score(y_val, y_hat))\n",
    "\n",
    "model_softr = VotingClassifier(estimators=estimators1, voting='soft')\n",
    "model_softr.fit(X_train_r,y_train)\n",
    "y_hat = model_softr.predict(X_val_r)\n",
    "print('accuracy for dataset without chosen columns: ', accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for whole dataset:  0.9319727891156463\n",
      "accuracy for dataset without chosen columns:  0.9251700680272109\n"
     ]
    }
   ],
   "source": [
    "model_soft = VotingClassifier(estimators=estimators1, voting='soft', weights=[0.2, 0.2, 0.6])\n",
    "model_soft.fit(X_train,y_train)\n",
    "y_hat = model_soft.predict(X_val)\n",
    "print('accuracy for whole dataset: ', accuracy_score(y_val, y_hat))\n",
    "\n",
    "model_softr = VotingClassifier(estimators=estimators1, voting='soft')\n",
    "model_softr.fit(X_train_r,y_train)\n",
    "y_hat = model_softr.predict(X_val_r)\n",
    "print('accuracy for dataset without chosen columns: ', accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for whole dataset:  0.9421768707482994\n",
      "accuracy for dataset without chosen columns:  0.9251700680272109\n"
     ]
    }
   ],
   "source": [
    "model_soft = VotingClassifier(estimators=estimators1, voting='soft', weights=[0.1, 0.1, 0.80])\n",
    "model_soft.fit(X_train,y_train)\n",
    "y_hat = model_soft.predict(X_val)\n",
    "print('accuracy for whole dataset: ', accuracy_score(y_val, y_hat))\n",
    "\n",
    "model_softr = VotingClassifier(estimators=estimators1, voting='soft')\n",
    "model_softr.fit(X_train_r,y_train)\n",
    "y_hat = model_softr.predict(X_val_r)\n",
    "print('accuracy for dataset without chosen columns: ', accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for whole dataset:  0.9285714285714286\n",
      "accuracy for dataset without chosen columns:  0.9081632653061225\n"
     ]
    }
   ],
   "source": [
    "model_soft = VotingClassifier(estimators=estimators2, voting='soft', weights=[0.1, 0.3, 0.3, 0.3])\n",
    "model_soft.fit(X_train,y_train)\n",
    "y_hat = model_soft.predict(X_val)\n",
    "print('accuracy for whole dataset: ', accuracy_score(y_val, y_hat))\n",
    "\n",
    "model_softr = VotingClassifier(estimators=estimators2, voting='soft')\n",
    "model_softr.fit(X_train_r,y_train)\n",
    "y_hat = model_softr.predict(X_val_r)\n",
    "print('accuracy for dataset without chosen columns: ', accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for whole dataset:  0.9251700680272109\n",
      "accuracy for dataset without chosen columns:  0.9115646258503401\n"
     ]
    }
   ],
   "source": [
    "model_soft = VotingClassifier(estimators=estimators2, voting='soft', weights=[0.1, 0.2, 0.5, 0.4])\n",
    "model_soft.fit(X_train,y_train)\n",
    "y_hat = model_soft.predict(X_val)\n",
    "print('accuracy for whole dataset: ', accuracy_score(y_val, y_hat))\n",
    "\n",
    "model_softr = VotingClassifier(estimators=estimators2, voting='soft')\n",
    "model_softr.fit(X_train_r,y_train)\n",
    "y_hat = model_softr.predict(X_val_r)\n",
    "print('accuracy for dataset without chosen columns: ', accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While testing different combinations of weights for soft voting we can observe the best result for \n",
    "\n",
    "estimators1=[('RandomForest', model2),('KNN', model4), ('SVC', model3)]\n",
    "\n",
    "with weights=[0.1, 0.1, 0.80]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9365079365079365"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = StackingClassifier(estimators=estimators1, final_estimator=LogisticRegression())\n",
    "clf.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9126984126984127"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = StackingClassifier(estimators=estimators, final_estimator=KNeighborsClassifier())\n",
    "clf.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = StackingClassifier(estimators=estimators2, final_estimator=KNeighborsClassifier())\n",
    "clf.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9523809523809523"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = StackingClassifier(estimators=estimators2, final_estimator=RandomForestClassifier())\n",
    "clf.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = StackingClassifier(estimators=estimators2, final_estimator=SVC())\n",
    "clf.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN - Best: 0.911224 using {'n_neighbors': 5, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "weights=['uniform', 'distance']\n",
    "n_neighbors=[1, 3, 5]\n",
    "param_grid = dict(weights=weights, n_neighbors=n_neighbors)\n",
    "\n",
    "knn= KNeighborsClassifier()\n",
    "grid = GridSearchCV(estimator=knn, param_grid=param_grid, cv = 10, scoring=['accuracy', 'roc_auc_ovo_weighted', 'f1_weighted'], n_jobs=-1, refit='accuracy')\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "print(\"KNN - Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Best: 0.970408 using {'C': 1, 'gamma': 0.5, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "kernel=['linear', 'rbf']\n",
    "C=[0.5, 1, 5, 20]\n",
    "gamma=[0.5, 1, 5, 10]\n",
    "param_grid = dict(kernel=kernel, C=C, gamma=gamma)\n",
    "\n",
    "svm= SVC(probability=True)\n",
    "grid = GridSearchCV(estimator=svm, param_grid=param_grid, cv = 5, scoring=['accuracy', 'roc_auc_ovo_weighted', 'f1_weighted'], n_jobs=-1, refit='accuracy')\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "print(\"SVM - Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - not all columns - Best: 0.965306 using {'C': 5, 'gamma': 0.5, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "kernel=['linear', 'rbf']\n",
    "C=[0.5, 1, 5, 20]\n",
    "gamma=[0.5, 1, 5, 10]\n",
    "param_grid = dict(kernel=kernel, C=C, gamma=gamma)\n",
    "\n",
    "svm= SVC(probability=True)\n",
    "grid = GridSearchCV(estimator=svm, param_grid=param_grid, cv = 5, scoring=['accuracy', 'roc_auc_ovo_weighted', 'f1_weighted'], n_jobs=-1, refit='accuracy')\n",
    "grid_result = grid.fit(X_train_r, y_train)\n",
    "print(\"SVM - not all columns - Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Best: 0.883673 using {'learning_rate': 0.15, 'max_depth': 7, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "max_depth = [3, 4, 5, 7]\n",
    "n_estimators = [50, 75, 100, 150, 200]\n",
    "learning_rate = [0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "param_grid = dict(max_depth=max_depth, n_estimators = n_estimators, learning_rate=learning_rate)\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "grid = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "print(\"XGBoost - Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN - Best: 0.914286 using {'weights': 'distance', 'n_neighbors': 9}\n"
     ]
    }
   ],
   "source": [
    "weights=['uniform', 'distance']\n",
    "n_neighbors=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "param_grid = dict(weights=weights, n_neighbors=n_neighbors)\n",
    "\n",
    "knn= KNeighborsClassifier()\n",
    "random = RandomizedSearchCV(estimator=knn, param_distributions=param_grid, cv = 10, scoring=['accuracy','roc_auc_ovo_weighted', 'f1_weighted'], n_jobs=-1, refit='accuracy')\n",
    "random_result = random.fit(X_train, y_train)\n",
    "print(\"KNN - Best: %f using %s\" % (random_result.best_score_, random_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Best: 0.966327 using {'C': 3.8029793634382836, 'gamma': 0.7561077817534814, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "kernel=['linear', 'rbf']\n",
    "C=uniform(0.1, 10)\n",
    "gamma=uniform(0.01, 1.0)\n",
    "param_grid = dict(kernel=kernel, C=C, gamma=gamma)\n",
    "\n",
    "svm= SVC(probability=True)\n",
    "random = RandomizedSearchCV(estimator=svm, param_distributions=param_grid, cv = 10, scoring=['accuracy','roc_auc_ovo_weighted', 'f1_weighted'], n_jobs=-1, refit='accuracy')\n",
    "random_result = random.fit(X_train, y_train)\n",
    "print(\"SVM - Best: %f using %s\" % (random_result.best_score_, random_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do :\n",
    "- szukanie hiperparametrow dla knn (done) i random tree (to be done)\n",
    "- stacking\n",
    "- autoML - TPOT & H20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ef1a4a969c4a96a88d7ab0363da1d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/180 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.9071428571428571\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.9071428571428571\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.9071428571428571\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.9071428571428571\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.9081632653061223\n",
      "\n",
      "Best pipeline: KNeighborsClassifier(input_matrix, n_neighbors=13, p=2, weights=distance)\n",
      "0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "\n",
    "tpot = TPOTClassifier(generations=5, population_size=30, verbosity=2)\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print(tpot.score(X_val, y_val))\n",
    "\n",
    "#tpot.export('tpot_best_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h2o\n",
      "  Downloading h2o-3.46.0.1-py2.py3-none-any.whl (265.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.6/265.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/Karolina/opt/anaconda3/lib/python3.9/site-packages (from h2o) (2.28.1)\n",
      "Requirement already satisfied: tabulate in /Users/Karolina/opt/anaconda3/lib/python3.9/site-packages (from h2o) (0.8.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Karolina/opt/anaconda3/lib/python3.9/site-packages (from requests->h2o) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/Karolina/opt/anaconda3/lib/python3.9/site-packages (from requests->h2o) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/Karolina/opt/anaconda3/lib/python3.9/site-packages (from requests->h2o) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Karolina/opt/anaconda3/lib/python3.9/site-packages (from requests->h2o) (2022.9.24)\n",
      "Installing collected packages: h2o\n",
      "Successfully installed h2o-3.46.0.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "h2o.init()\n",
    "\n",
    "mobile_df_train = pd.concat([X_train, y_train], axis=1)\n",
    "mobile_df_val =  pd.concat([X_val, y_val], axis=1)\n",
    "mobile_df_test = pd.concat([X_test, y_test], axis=1)\n",
    "train_h2o = h2o.H2OFrame(mobile_df_train)\n",
    "val_h2o = h2o.H2OFrame(mobile_df_val)\n",
    "test_h2o = h2o.H2OFrame(mobile_df_test)\n",
    "train_h2o['price_range'] = train_h2o['price_range'].asfactor()\n",
    "\n",
    "aml = H2OAutoML(max_models=15, seed=1, sort_metric = 'accuracy')\n",
    "aml.train(x=X_train.columns.tolist(), y='price_range', training_frame=train_h2o, validation_frame=val_h2o)\n",
    "\n",
    "lb = aml.leaderboard\n",
    "lb.head(rows=lb.nrows) \n",
    "\n",
    "perf = aml.leader.model_performance(test_data=test_h2o)\n",
    "m = h2o.get_model(\"StackedEnsemble_BestOfFamily_1_AutoML_1_20240419_130852\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = h2o.get_model(\"StackedEnsemble_BestOfFamily_1_AutoML_1_20240419_130852\")\n",
    "\n",
    "#h2o.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id                                                   accuracy    mean_per_class_error    logloss      rmse        mse\n",
      "XRT_1_AutoML_1_20240419_130852                             0.817347               0.182653    0.581563  0.442793  0.196065\n",
      "DRF_1_AutoML_1_20240419_130852                             0.818367               0.181633    0.572689  0.437783  0.191654\n",
      "GBM_grid_1_AutoML_1_20240419_130852_model_1                0.837755               0.162245    0.434699  0.36921   0.136316\n",
      "DeepLearning_1_AutoML_1_20240419_130852                    0.857143               0.142857    0.343905  0.324455  0.105271\n",
      "GBM_4_AutoML_1_20240419_130852                             0.859184               0.140816    0.366417  0.330534  0.109253\n",
      "XGBoost_2_AutoML_1_20240419_130852                         0.860204               0.139796    0.339111  0.32338   0.104575\n",
      "XGBoost_1_AutoML_1_20240419_130852                         0.864286               0.135714    0.353055  0.329554  0.108606\n",
      "GBM_2_AutoML_1_20240419_130852                             0.866327               0.133673    0.336623  0.317682  0.100922\n",
      "GBM_5_AutoML_1_20240419_130852                             0.871429               0.128571    0.350077  0.315528  0.0995577\n",
      "GBM_3_AutoML_1_20240419_130852                             0.873469               0.126531    0.341286  0.316327  0.100063\n",
      "XGBoost_3_AutoML_1_20240419_130852                         0.877551               0.122449    0.311383  0.305504  0.0933325\n",
      "XGBoost_grid_1_AutoML_1_20240419_130852_model_1            0.882653               0.117347    0.331092  0.314844  0.0991266\n",
      "XGBoost_grid_1_AutoML_1_20240419_130852_model_2            0.883673               0.116327    0.319184  0.309388  0.0957209\n",
      "GBM_1_AutoML_1_20240419_130852                             0.888776               0.111224    0.29214   0.296503  0.0879138\n",
      "GLM_1_AutoML_1_20240419_130852                             0.944898               0.055102    0.128392  0.197782  0.0391176\n",
      "StackedEnsemble_AllModels_1_AutoML_1_20240419_130852       0.95                   0.05        0.113658  0.185915  0.0345643\n",
      "StackedEnsemble_BestOfFamily_1_AutoML_1_20240419_130852    0.953061               0.0469388   0.110806  0.182282  0.0332268\n",
      "[17 rows x 6 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lime\n",
    "#!pip install pdpbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "#from pdpbox import pdp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn .inspection import PartialDependenceDisplay\n",
    "\n",
    "svm = SVC(gamma=0.5, C=1, kernel='linear', probability=True)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,\n",
    "                                                   mode='classification',\n",
    "                                                   feature_names=X_train.columns.tolist(),\n",
    "                                                   class_names=y_train.unique(),\n",
    "                                                   discretize_continuous=True)\n",
    "\n",
    "# Przygotowanie pojedynczej obserwacji do analizy\n",
    "observation = X_train.iloc[3]\n",
    "\n",
    "# Wyjaśnienie predykcji modelu SVC\n",
    "predictions = svm.predict_proba([observation])\n",
    "classifier_fn = lambda x: predictions\n",
    "explanation = explainer.explain_instance(data_row=observation, predict_fn=svm.predict_proba, num_features=20)\n",
    "\n",
    "# Wyświetlenie wyjaśnienia LIME\n",
    "explanation.show_in_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = 0\n",
    "feature_index = [0] \n",
    "display = PartialDependenceDisplay.from_estimator(svm, X_train, features=feature_index, grid_resolution=50, target=target_class)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 1.22 or greater. Got NumPy 1.21.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d1/hw8k8s1x019_54t0zwhhcrxr0000gp/T/ipykernel_32504/3537938928.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/shap/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_explanation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCohorts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExplanation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# explainers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_additive\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdditiveExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/shap/_explanation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mslicer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mObj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSlicer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDimensionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_general\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/shap/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from ._clustering import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdelta_minimization_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mhclust\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhclust_ordering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpartition_tree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/shap/utils/_clustering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_show_progress\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numba/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0m_ensure_critical_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;31m# END DO NOT MOVE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# ---------------------- WARNING WARNING WARNING ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numba/__init__.py\u001b[0m in \u001b[0;36m_ensure_critical_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         msg = (f\"Numba needs NumPy 1.22 or greater. Got NumPy \"\n\u001b[1;32m     39\u001b[0m                f\"{numpy_version[0]}.{numpy_version[1]}.\")\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnumpy_version\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Numba needs NumPy 1.26 or less\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Numba needs NumPy 1.22 or greater. Got NumPy 1.21."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "import shap\n",
    "\n",
    "svm = SVC(gamma=0.5, C=1, kernel='linear', probability=True)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Użyj biblioteki SHAP do obliczenia Partial Dependence Shapley Values\n",
    "explainer = shap.Explainer(svm, X_train)\n",
    "shap_values = explainer(X_train)\n",
    "\n",
    "# Wygeneruj wykresy Partial Dependence Plots\n",
    "shap.plots.partial_dependence(shap_values, features=['RM', 'LSTAT'])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
