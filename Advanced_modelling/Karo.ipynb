{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(23)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobile_modelling = pd.read_csv('../Data/Data_modelling/mobile_modelling.csv')\n",
    "\n",
    "X = mobile_modelling.iloc[:, 0:-1]\n",
    "y = mobile_modelling.iloc[:, -1]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val, y_val, stratify=y_val, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "mobile_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# more than 0.02\n",
    "columns_to_drop_1 = ['touch_screen', 'blue', 'dual_sim', 'four_g', 'wifi', 'three_g']\n",
    "X_train_r = X_train.drop(columns=columns_to_drop_1)\n",
    "X_val_r = X_val.drop(columns=columns_to_drop_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into consideration our first modelling and the results we obtained we plan to take into consideration 3 simple models that performed well on our data:\n",
    "- SVC (for hard voting) / KNN (for soft voting)\n",
    "- Decision Tree\n",
    "- Random Forest Classifier\n",
    "\n",
    "We will use soft voting as well as hard voting. We will also try to find the best weight combinations for chosen models combination since we think that focusing on the model with the best performance may be beneficial. We will check the results for data without removed columns and with removed columns that have feature importance greater than 0,02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# used models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model1 = DecisionTreeClassifier(random_state=1)\n",
    "model2 = RandomForestClassifier()\n",
    "model3 = SVC(random_state=1, max_iter=1000, probability=True)\n",
    "model33 = SVC(random_state=1, max_iter=1000)\n",
    "model4 = KNeighborsClassifier()\n",
    "estimators=[('DecisionTree', model1), ('RandomForest', model2), ('SVC', model33)]\n",
    "estimators1=[('RandomForest', model2), ('KNN', model4), ('SVC', model3)]\n",
    "estimators2=[('DecisionTree', model1), ('RandomForest', model2),('SVC', model3), ('KNN', model4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works for 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for whole dataset:  0.9115646258503401\n",
      "accuracy for dataset without chosen columns:  0.9013605442176871\n"
     ]
    }
   ],
   "source": [
    "model_hard = VotingClassifier(estimators=estimators, voting='hard')\n",
    "model_hard.fit(X_train,y_train)\n",
    "y_hat = model_hard.predict(X_val)\n",
    "print('accuracy for whole dataset: ', accuracy_score(y_val, y_hat))\n",
    "\n",
    "model_hardr = VotingClassifier(estimators=estimators, voting='hard')\n",
    "model_hardr.fit(X_train_r,y_train)\n",
    "y_hat = model_hardr.predict(X_val_r)\n",
    "print('accuracy for dataset without chosen columns: ', accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for whole dataset:  0.9047619047619048\n",
      "accuracy for dataset without chosen columns:  0.9081632653061225\n"
     ]
    }
   ],
   "source": [
    "model_hard = VotingClassifier(estimators=estimators2, voting='hard')\n",
    "model_hard.fit(X_train,y_train)\n",
    "y_hat = model_hard.predict(X_val)\n",
    "print('accuracy for whole dataset: ', accuracy_score(y_val, y_hat))\n",
    "\n",
    "model_hardr = VotingClassifier(estimators=estimators2, voting='hard')\n",
    "model_hardr.fit(X_train_r,y_train)\n",
    "y_hat = model_hardr.predict(X_val_r)\n",
    "print('accuracy for dataset without chosen columns: ', accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for whole dataset:  0.9047619047619048\n",
      "accuracy for dataset without chosen columns:  0.9081632653061225\n"
     ]
    }
   ],
   "source": [
    "model_soft = VotingClassifier(estimators=estimators1, voting='soft')\n",
    "model_soft.fit(X_train,y_train)\n",
    "y_hat = model_hard.predict(X_val)\n",
    "print('accuracy for whole dataset: ', accuracy_score(y_val, y_hat))\n",
    "\n",
    "model_softr = VotingClassifier(estimators=estimators1, voting='soft')\n",
    "model_softr.fit(X_train_r,y_train)\n",
    "y_hat = model_hardr.predict(X_val_r)\n",
    "print('accuracy for dataset without chosen columns: ', accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for whole dataset:  0.9047619047619048\n",
      "accuracy for dataset without chosen columns:  0.9081632653061225\n"
     ]
    }
   ],
   "source": [
    "model_soft = VotingClassifier(estimators=estimators2, voting='soft')\n",
    "model_soft.fit(X_train,y_train)\n",
    "y_hat = model_hard.predict(X_val)\n",
    "print('accuracy for whole dataset: ', accuracy_score(y_val, y_hat))\n",
    "\n",
    "model_softr = VotingClassifier(estimators=estimators1, voting='soft')\n",
    "model_softr.fit(X_train_r,y_train)\n",
    "y_hat = model_hardr.predict(X_val_r)\n",
    "print('accuracy for dataset without chosen columns: ', accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for whole dataset:  0.9047619047619048\n",
      "accuracy for dataset without chosen columns:  0.9081632653061225\n"
     ]
    }
   ],
   "source": [
    "model_soft = VotingClassifier(estimators=estimators1, voting='soft', weights=[0.2, 0.2, 0.6])\n",
    "model_soft.fit(X_train,y_train)\n",
    "y_hat = model_hard.predict(X_val)\n",
    "print('accuracy for whole dataset: ', accuracy_score(y_val, y_hat))\n",
    "\n",
    "model_softr = VotingClassifier(estimators=estimators1, voting='soft')\n",
    "model_softr.fit(X_train_r,y_train)\n",
    "y_hat = model_hardr.predict(X_val_r)\n",
    "print('accuracy for dataset without chosen columns: ', accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for whole dataset:  0.9047619047619048\n",
      "accuracy for dataset without chosen columns:  0.9081632653061225\n"
     ]
    }
   ],
   "source": [
    "model_soft = VotingClassifier(estimators=estimators1, voting='soft', weights=[0.1, 0.1, 0.80])\n",
    "model_soft.fit(X_train,y_train)\n",
    "y_hat = model_hard.predict(X_val)\n",
    "print('accuracy for whole dataset: ', accuracy_score(y_val, y_hat))\n",
    "\n",
    "model_softr = VotingClassifier(estimators=estimators1, voting='soft')\n",
    "model_softr.fit(X_train_r,y_train)\n",
    "y_hat = model_hardr.predict(X_val_r)\n",
    "print('accuracy for dataset without chosen columns: ', accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for whole dataset:  0.9047619047619048\n",
      "accuracy for dataset without chosen columns:  0.9081632653061225\n"
     ]
    }
   ],
   "source": [
    "model_soft = VotingClassifier(estimators=estimators2, voting='soft', weights=[0.1, 0.3, 0.3, 0.3])\n",
    "model_soft.fit(X_train,y_train)\n",
    "y_hat = model_hard.predict(X_val)\n",
    "print('accuracy for whole dataset: ', accuracy_score(y_val, y_hat))\n",
    "\n",
    "model_softr = VotingClassifier(estimators=estimators2, voting='soft')\n",
    "model_softr.fit(X_train_r,y_train)\n",
    "y_hat = model_hardr.predict(X_val_r)\n",
    "print('accuracy for dataset without chosen columns: ', accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for whole dataset:  0.9047619047619048\n",
      "accuracy for dataset without chosen columns:  0.9081632653061225\n"
     ]
    }
   ],
   "source": [
    "model_soft = VotingClassifier(estimators=estimators2, voting='soft', weights=[0.1, 0.2, 0.5, 0.4])\n",
    "model_soft.fit(X_train,y_train)\n",
    "y_hat = model_hard.predict(X_val)\n",
    "print('accuracy for whole dataset: ', accuracy_score(y_val, y_hat))\n",
    "\n",
    "model_softr = VotingClassifier(estimators=estimators2, voting='soft')\n",
    "model_softr.fit(X_train_r,y_train)\n",
    "y_hat = model_hardr.predict(X_val_r)\n",
    "print('accuracy for dataset without chosen columns: ', accuracy_score(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While testing different combinations of weights for soft voting we always observed the same accuracy which is very surprising anda bit counterintuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "kernel=['linear', 'rbf']\n",
    "C=[0.5, 1, 5, 20, 100]\n",
    "gamma=[0.5, 1, 5, 10, 50]\n",
    "param_grid = dict(kernel=kernel, C=C, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.965307 using {'C': 0.5, 'gamma': 0.5, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "svm= SVC(probability=True)\n",
    "grid = GridSearchCV(estimator=svm, param_grid=param_grid, cv = 3, scoring=['accuracy', 'roc_auc_ovo_weighted', 'f1_weighted'], n_jobs=-1, refit='accuracy')\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "kernel=['linear', 'rbf']\n",
    "C=uniform(0.1, 10)\n",
    "gamma=uniform(0.01, 1.0)\n",
    "param_grid = dict(kernel=kernel, C=C, gamma=gamma)\n",
    "\n",
    "svm= SVC()\n",
    "random = RandomizedSearchCV(estimator=svm, param_distributions=param_grid, cv = 5, scoring=['accuracy','roc_auc_ovo_weighted', 'f1_weighted'], n_jobs=-1, refit='accuracy')\n",
    "random_result = grid.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svm = SVC()\n",
    "\n",
    "# w teorii przy grid/random searchu juz sie robi crossvalidation\n",
    "# mozna wsm dopytac jak to wyglada z ta crossvalidacja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do :\n",
    "- szukanie hiperparametrow dla knn i random tree ?\n",
    "- czy crossvalidation potrzebne jak przy hiperparametrach jest - napisac do Tomaszewskiej\n",
    "- stacking - dodac z regresja liniowa i naive bayes\n",
    "-  autoML - TPOT & H20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
